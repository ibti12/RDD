Spark RDD â€“ Lab 2

Ce dÃ©pÃ´t regroupe lâ€™ensemble des scripts rÃ©alisÃ©s dans le cadre du Lab 2 dÃ©diÃ© aux RDD (Resilient Distributed Datasets) avec Apache Spark.
Lâ€™objectif est de comprendre en profondeur le fonctionnement des RDD, leurs transformations, leurs actions et les mÃ©canismes dâ€™optimisation.

ğŸš€ Objectifs du Lab

Comprendre la structure et le rÃ´le des RDD dans Spark.

Manipuler les transformations essentielles : map, flatMap, filter, reduceByKey, etc.

Travailler avec les Pair RDDs (RDDs clÃ©-valeur).

GÃ©rer le partitionnement personnalisÃ© et analyser son impact.

Explorer la persistance, le caching et leurs performances.

DÃ©tecter et corriger les problÃ¨mes de skew (dÃ©sÃ©quilibre des donnÃ©es).

DÃ©velopper une intuition sur la performance des traitements distribuÃ©s.

ğŸ“ Contenu du Lab 2
ğŸ”¹ Exploration & Manipulation

lab2_explore_data.py â€” DÃ©couverte du dataset + opÃ©rations basiques.

lab2_filter.py â€” Filtrage simple et avancÃ©.

lab2_map_operations.py â€” Transformations courantes (map, flatMapâ€¦).

lab2_map_practice.py â€” Exercices pratiques.

ğŸ”¹ Pair RDD & Joins

lab2_keyvalue.py â€” Manipulations clÃ©-valeur (pair RDD).

lab2_joins.py â€” DiffÃ©rents types de jointures entre RDDs.

ğŸ”¹ Partitionnement & Optimisation

lab2_partitions.py â€” Analyse, crÃ©ation et optimisation des partitions.

lab2_custom_partitioner.py â€” ImplÃ©mentation dâ€™un partitionneur personnalisÃ©.

lab2_skew.py â€” DÃ©tection et correction du data skew.

lab2_performance_challenge.py â€” DÃ©fi de performance (approche + rÃ©solution).

ğŸ”¹ Persistance

lab2_persistence.py â€” Cache, persist, memory/disk, et impact rÃ©el.

ğŸ”¹ Outputs

lab2_output.txt â€” RÃ©sultats produits par certains scripts.

ğŸ› ï¸ Technologies utilisÃ©es

PySpark

Apache Spark

Python 3.x

ğŸ“¦ PrÃ©requis

Avant dâ€™exÃ©cuter les scripts :

pip install pyspark


Assurez-vous Ã©galement que Java (JDK 8 ou +) est installÃ©.

â–¶ï¸ ExÃ©cution dâ€™un script
spark-submit lab2_map_operations.py


Ou dans un notebook :

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("RDD-Lab2").getOrCreate()

ğŸ“š Ce que vous apprendrez

âœ” Les bases solides des RDD
âœ” Les transformations clÃ©s et leurs comportements
âœ” Le fonctionnement interne du partitionnement
âœ” Les bonnes pratiques de performance Spark
âœ” La gestion du caching et de la persistance
âœ” La manipulation des donnÃ©es distribuÃ©es Ã  grande Ã©chelle
